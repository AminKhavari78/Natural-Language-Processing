{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "InlxqIWEPufX",
        "nbEDaD1znIx_",
        "47TPAvx-xTZu",
        "h3Fn7_50yYBp",
        "Y9nGz4z70z4H",
        "k-oHAjfi5PFd"
      ],
      "authorship_tag": "ABX9TyMb0Fpp6rOtvwqr6krir9cl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AminKhavari78/Natural-Language-Processing/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementing the Transformer encoder\n"
      ],
      "metadata": {
        "id": "InlxqIWEPufX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.layers import ( Activation, Dense, ReLU,\n",
        "                          LayerNormalization, MultiHeadAttention, Dropout,\n",
        "                           Embedding)"
      ],
      "metadata": {
        "id": "9Hl5lUNmQTa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Position-wise Feed-Forward Networks\n",
        "### This consists of two linear transformations with a ReLU activation in between.\n",
        "###FFN(x) = max(0, xW1 + b1 )W2 + b2\n"
      ],
      "metadata": {
        "id": "nbEDaD1znIx_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVaxvTWROvOQ"
      },
      "outputs": [],
      "source": [
        "class FeedForward(layers.Layer):\n",
        "    def __init__(self, d_ff, d_model, **kwargs):\n",
        "        super().__init__()\n",
        "        self.fully_connected1 = Dense(d_ff)  # First fully connected layer\n",
        "        self.fully_connected2 = Dense(d_model)  # Second fully connected layer\n",
        "        self.activation = ReLU()  # ReLU activation layer\n",
        "\n",
        "\n",
        "    def call(self, x):\n",
        "        # The input is passed into the two fully-connected layers, with a ReLU in between\n",
        "        x_fc1 = self.fully_connected1(x)\n",
        "\n",
        "        return self.fully_connected2(self.activation(x_fc1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself."
      ],
      "metadata": {
        "id": "47TPAvx-xTZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AddNormalization(layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        self.layer_norm = LayerNormalization()  # Layer normalization layer\n",
        "\n",
        "\n",
        "    def call(self, x, sublayer_x):\n",
        "        # The sublayer input and output need to be of the same shape to be summed\n",
        "        add = x + sublayer_x\n",
        "\n",
        "        # Apply layer normalization to the sum\n",
        "        return self.layer_norm(add)"
      ],
      "metadata": {
        "id": "r14g2bjdQ-Wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network."
      ],
      "metadata": {
        "id": "h3Fn7_50yYBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
        "        super(EncoderLayer, self).__init__(**kwargs)\n",
        "        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.add_norm1 = AddNormalization()\n",
        "        self.feed_forward = FeedForward(d_ff, d_model)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "        self.add_norm2 = AddNormalization()\n",
        "\n",
        "\n",
        "    def call(self, x, padding_mask, training):\n",
        "        # Multi-head attention layer\n",
        "        multihead_output = self.multihead_attention(x, x, x, padding_mask)\n",
        "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
        "\n",
        "        # Add in a dropout layer\n",
        "        multihead_output = self.dropout1(multihead_output, training=training)\n",
        "\n",
        "        # Followed by an Add & Norm layer\n",
        "        addnorm_output = self.add_norm1(x, multihead_output)\n",
        "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
        "\n",
        "        # Followed by a fully connected layer\n",
        "        feedforward_output = self.feed_forward(addnorm_output)\n",
        "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
        "\n",
        "        # Add in another dropout layer\n",
        "        feedforward_output = self.dropout2(feedforward_output, training=training)\n",
        "\n",
        "        # Followed by another Add & Norm layer\n",
        "        return self.add_norm2(addnorm_output, feedforward_output)"
      ],
      "metadata": {
        "id": "fvCeqnWNRQzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Input Embedding and Positional Encoding"
      ],
      "metadata": {
        "id": "Y9nGz4z70z4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionEmbeddingFixedWeights(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)\n",
        "        position_embedding_matrix = self.get_position_encoding(sequence_length, output_dim)\n",
        "        self.word_embedding_layer = Embedding(\n",
        "            input_dim=vocab_size, output_dim=output_dim,\n",
        "            weights=[word_embedding_matrix],\n",
        "            trainable=False\n",
        "        )\n",
        "        self.position_embedding_layer = Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim,\n",
        "            weights=[position_embedding_matrix],\n",
        "            trainable=False\n",
        "        )\n",
        "\n",
        "    def get_position_encoding(self, seq_len, d, n=10000):\n",
        "        P = np.zeros((seq_len, d))\n",
        "        for k in range(seq_len):\n",
        "            for i in np.arange(int(d/2)):\n",
        "                denominator = np.power(n, 2*i/d)\n",
        "                P[k, 2*i] = np.sin(k/denominator)\n",
        "                P[k, 2*i+1] = np.cos(k/denominator)\n",
        "        return P\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
        "        embedded_words = self.word_embedding_layer(inputs)\n",
        "        embedded_indices = self.position_embedding_layer(position_indices)\n",
        "        return embedded_words + embedded_indices"
      ],
      "metadata": {
        "id": "MuVAezaWTmC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(layers.Layer):\n",
        "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n",
        "        self.dropout = Dropout(rate)\n",
        "        self.encoder_layer = [EncoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
        "\n",
        "\n",
        "    def call(self, input_sentence, padding_mask, training):\n",
        "        # Generate the positional encoding\n",
        "        pos_encoding_output = self.pos_encoding(input_sentence)\n",
        "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
        "\n",
        "        # Add in a dropout layer\n",
        "        x = self.dropout(pos_encoding_output, training=training)\n",
        "\n",
        "        # Pass on the positional encoded values to each encoder layer\n",
        "        for i, layer in enumerate(self.encoder_layer):\n",
        "            x = layer(x, padding_mask, training)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "tBjjAdKQRXk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h = 8  # Number of self-attention heads\n",
        "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
        "d_v = 64  # Dimensionality of the linearly projected values\n",
        "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
        "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
        "n = 6  # Number of layers in the encoder stack\n",
        "\n",
        "batch_size = 64  # Batch size from the training process\n",
        "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
        "\n",
        "enc_vocab_size = 20 # Vocabulary size for the encoder\n",
        "input_seq_length = 5  # Maximum length of the input sequence\n",
        "\n",
        "input_seq = np.random.random((batch_size, input_seq_length))\n",
        "\n",
        "encoder = Encoder(enc_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
        "#print(encoder(input_seq, None, True))\n"
      ],
      "metadata": {
        "id": "rj9NQU4ZSYWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.build"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CC99qQ51XwSA",
        "outputId": "b8c4c5e6-9f18-494d-c2ba-20907635dbe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Layer.build of <__main__.Encoder object at 0x7de6ce1310c0>>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementing the Transformer Decoder"
      ],
      "metadata": {
        "id": "k-oHAjfi5PFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.multihead_attention1 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.add_norm1 = AddNormalization()\n",
        "        self.multihead_attention2 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "        self.add_norm2 = AddNormalization()\n",
        "        self.feed_forward = FeedForward(d_ff, d_model)\n",
        "        self.dropout3 = Dropout(rate)\n",
        "        self.add_norm3 = AddNormalization()\n",
        "\n",
        "\n",
        "    def call(self, x, encoder_output, lookahead_mask, padding_mask, training):\n",
        "        # Multi-head attention layer\n",
        "        multihead_output1 = self.multihead_attention1(x, x, x, lookahead_mask)\n",
        "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
        "\n",
        "        # Add in a dropout layer\n",
        "        multihead_output1 = self.dropout1(multihead_output1, training=training)\n",
        "\n",
        "        # Followed by an Add & Norm layer\n",
        "        addnorm_output1 = self.add_norm1(x, multihead_output1)\n",
        "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
        "\n",
        "        # Followed by another multi-head attention layer\n",
        "        multihead_output2 = self.multihead_attention2(addnorm_output1, encoder_output, encoder_output, padding_mask)\n",
        "\n",
        "        # Add in another dropout layer\n",
        "        multihead_output2 = self.dropout2(multihead_output2, training=training)\n",
        "\n",
        "        # Followed by another Add & Norm layer\n",
        "        addnorm_output2 = self.add_norm1(addnorm_output1, multihead_output2)\n",
        "\n",
        "        # Followed by a fully connected layer\n",
        "        feedforward_output = self.feed_forward(addnorm_output2)\n",
        "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
        "\n",
        "        # Add in another dropout layer\n",
        "        feedforward_output = self.dropout3(feedforward_output, training=training)\n",
        "\n",
        "        # Followed by another Add & Norm layer\n",
        "        return self.add_norm3(addnorm_output2, feedforward_output)\n"
      ],
      "metadata": {
        "id": "07D0qUuJ5R69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Decoder(layers.Layer):\n",
        "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
        "        super(Decoder, self).__init__(**kwargs)\n",
        "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n",
        "        self.dropout = Dropout(rate)\n",
        "        self.decoder_layer = [DecoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
        "\n",
        "\n",
        "    def call(self, output_target, encoder_output, lookahead_mask, padding_mask, training):\n",
        "        # Generate the positional encoding\n",
        "        pos_encoding_output = self.pos_encoding(output_target)\n",
        "        # Expected output shape = (number of sentences, sequence_length, d_model)\n",
        "\n",
        "        # Add in a dropout layer\n",
        "        x = self.dropout(pos_encoding_output, training=training)\n",
        "\n",
        "        # Pass on the positional encoded values to each encoder layer\n",
        "        for i, layer in enumerate(self.decoder_layer):\n",
        "            x = layer(x, encoder_output, lookahead_mask, padding_mask, training)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "BcPZwHi45z0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h = 8  # Number of self-attention heads\n",
        "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
        "d_v = 64  # Dimensionality of the linearly projected values\n",
        "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
        "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
        "n = 6  # Number of layers in the encoder stack\n",
        "\n",
        "batch_size = 64  # Batch size from the training process\n",
        "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
        "\n",
        "dec_vocab_size = 20 # Vocabulary size for the decoder\n",
        "input_seq_length = 5  # Maximum length of the input sequence\n",
        "\n",
        "input_seq = np.random.random((batch_size, input_seq_length))\n",
        "enc_output = np.random.random((batch_size, input_seq_length, d_model))\n",
        "\n",
        "\n",
        "decoder = Decoder(dec_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
        "decoder.build\n",
        "#print(decoder(input_seq, enc_output, None, True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1q_Y7ax50Ag",
        "outputId": "1541eea7-87e4-47d8-c4f4-f1c8158831e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Layer.build of <__main__.Decoder object at 0x7bb4e7917910>>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g7byqaax50J5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}